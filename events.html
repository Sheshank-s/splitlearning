<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Events and Efficiency</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">MIT Media Lab's Split Learning Project</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="blog.html">All Posts</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Events and Efficiency</h1>
                            <h2>Events</h2>

                            <h3>CVPR 2019 Half-Day Tutorial on “Distributed Private Machine Learning for Computer Vision: Federated Learning, Split Learning and Beyond”</h3>
                            <p>We gave a half-day tutorial at CVPR 2019: On Distributed Private Machine Learning for Computer Vision: Federated Learning, Split Learning and Beyond by <u><span style="font-weight: bold;">Brendan McMahan</span></u> (Google, USA), <u><span style="font-weight: bold;">Jakub Konečný</span></u> (Google, USA), Otkrist Gupta (LendBuzz), <u><span style="font-weight: bold;">Ramesh Raskar</span></u> (MIT Media Lab, Cambridge, Massachusetts, USA),<u><span style="font-weight: bold;">Hassan Takabi</span></u> (University of North Texas, Texas, USA) and <u><span style="font-weight: bold;">Praneeth Vepakomma</span></u> (MIT Media Lab, Cambridge, Massachusetts, USA).
                            <br> Find more information at this <a href="https://nopeekcvpr.github.io/">link</a>.</p>

                            <h3>Talk on Split Learning at Datacouncil.ai SF 2019</h3>
                            <p>Find the slides <a href="https://splitlearning.github.io/splitlearning.github.io/Split-Learning-A-Resource-Efficient-Distributed-Deep-Learning-Method-Without-Sensitive-Data-Sharing.pdf">here</a>.</p>
                            
                            <h3>Course on Advances in imaging and machine learning: Medical, VR-AR, And Self-Driving Cars</h3>
                            <p>Find out more information <a href="https://professional.mit.edu/programs/short-programs/advances-imaging">here</a>.</p>

                            <h2>Efficiency</h2>
                            
                            <h3>Split learning’s computational and communication efficiency on clients</h3>
                            <p>Client-side communication costs are significantly reduced as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. The client-side computation costs of learning the weights of the network are also significantly reduced for the same reason. In terms of model performance, the accuracies of Split NN remained competitive to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients as shown below in terms of teraflops of computation and gigabytes of communication when split learning is used to train Resnet and VGG architectures over 100 and 500 clients with CIFAR 10 and CIFAR 100 datasets.</p>
                            <div class="col-4"><span class="image fit"><img src="images/quantitativeresults.png"  style="width:75%; margin:auto;" alt="" /></span></div>
                            <div class="col-4"><span class="image fit"><img src="images/efficiency2.png"  style="width:75%; margin:auto;" alt="" /></span></div>

                            <h3>Versatile plug-and-play configurations of split learning</h3>
                            <p>Versatile configurations of split learning configurations cater to various practical settings of <u><span style="font-weight: bold;">i) multiple entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks, iii) learning without sharing labels, iv) multi-task split learning, v) multi-hop split learning</span></u> and other hybrid possibilities to name a few as shown below and further detailed in our paper <a href="https://arxiv.org/pdf/1812.00564.pdf">here (PDF)</a>. </p>
                            <div class="col-4"><span class="image fit"><img src="images/versatile1.png" style="width:75%; margin:auto;"  alt="" /></span></div>
                        </div>
					</section>

			</div>

		<!-- Footer -->
        <footer id="footer" class="wrapper style1-alt">
            <div class="inner">
                &copy; MIT Media Lab: Split Learning. All rights reserved.
                <br>Design: <a href="http://html5up.net">HTML5 UP</a>
            </div>
        </footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>